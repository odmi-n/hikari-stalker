import os
import zipfile
import logging
from pathlib import Path
from bs4 import BeautifulSoup
import re
from datetime import datetime
import json

# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger('edinet_unzipper')

# main.pyã‹ã‚‰å‘¼ã³å‡ºã—å¯èƒ½ãªé–¢æ•°
def parse_and_filter_reports(download_dir):
    """
    ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã€LINEé€šçŸ¥ç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    Args:
        download_dir: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    Returns:
        list: LINEé€šçŸ¥ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆ
    """
    # ãƒ‘ã‚¹è§£æ±º
    target_dir = Path(download_dir)
    if not target_dir.exists():
        logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_dir}")
        return []
    
    # ãƒ‘ãƒ¼ã‚µãƒ¼ã®åˆæœŸåŒ–
    parser = EdinetParser(target_dir)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ‡ãƒ¼ã‚¿ã‚’è§£æ
    all_results, new_results = parser.parse_directory()
    
    # é€šçŸ¥ç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ
    messages = []
    
    # æ–°è¦ã®å ±å‘Šæ›¸ã®ã¿å‡¦ç†
    if new_results:
        logger.info(f"{len(new_results)}ä»¶ã®æ–°è¦å ±å‘Šæ›¸ã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åŒ–ã—ã¾ã™")
        for result in new_results:
            # LINEç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
            line_message = parser.get_line_message(result)
            messages.append(line_message)
    else:
        logger.info("æ–°è¦ã®å ±å‘Šæ›¸ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    logger.info(f"åˆè¨ˆ{len(messages)}ä»¶ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
    return messages

class EdinetUnzipper:
    def __init__(self, target_dir=None):
        """
        åˆæœŸåŒ–
        Args:
            target_dir (str, optional): å‡¦ç†å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚æŒ‡å®šãŒãªã„å ´åˆã¯ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨ã€‚
        """
        self.target_dir = Path(target_dir) if target_dir else Path.cwd()
        logger.info(f"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.target_dir}")

    def find_zip_files(self):
        """
        å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        Returns:
            list: è¦‹ã¤ã‹ã£ãŸZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        zip_files = list(self.target_dir.glob("*.zip"))
        logger.info(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒ{len(zip_files)}å€‹è¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
        return zip_files

    def unzip_file(self, zip_path):
        """
        ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£å‡ã—ã€å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
        Args:
            zip_path (Path): è§£å‡å¯¾è±¡ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        Returns:
            bool: å‡¦ç†ãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            # è§£å‡å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ä½œæˆï¼ˆZIPãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰.zipã‚’é™¤ã„ãŸã‚‚ã®ï¼‰
            extract_dir = zip_path.parent / zip_path.stem
            
            # è§£å‡å‡¦ç†
            logger.info(f"è§£å‡é–‹å§‹: {zip_path}")
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(extract_dir)
            logger.info(f"è§£å‡å®Œäº†: {extract_dir}")

            # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            zip_path.unlink()
            logger.info(f"ZIPãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤å®Œäº†: {zip_path}")
            
            return True

        except zipfile.BadZipFile:
            logger.error(f"ä¸æ­£ãªZIPãƒ•ã‚¡ã‚¤ãƒ«: {zip_path}")
            return False
        except Exception as e:
            logger.error(f"è§£å‡å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return False

    def process_all_zips(self):
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
        Returns:
            tuple: (æˆåŠŸä»¶æ•°, å¤±æ•—ä»¶æ•°)
        """
        success_count = 0
        failure_count = 0

        zip_files = self.find_zip_files()
        if not zip_files:
            logger.info("å‡¦ç†å¯¾è±¡ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return success_count, failure_count

        for zip_path in zip_files:
            if self.unzip_file(zip_path):
                success_count += 1
            else:
                failure_count += 1

        logger.info(f"å‡¦ç†å®Œäº† - æˆåŠŸ: {success_count}ä»¶, å¤±æ•—: {failure_count}ä»¶")
        return success_count, failure_count

class EdinetParser:
    def __init__(self, base_dir):
        """
        åˆæœŸåŒ–
        Args:
            base_dir (str): è§£å‡ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ãŒæ ¼ç´ã•ã‚Œã¦ã„ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.base_dir = Path(base_dir)
        self.setup_logging()
        
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨
        try:
            from db import ReportDatabase
            self.db = ReportDatabase()
            self.logger.info("SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶šã—ã¾ã—ãŸ")
        except ImportError:
            self.logger.warning("db ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ãã¾ã›ã‚“ã€‚JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self.processed_reports_file = Path(base_dir).parent / "processed_reports.json"
            self.processed_reports = self.load_processed_reports()

    def setup_logging(self):
        """ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š"""
        self.logger = logging.getLogger('edinet_parser')
        if not self.logger.handlers:
            self.logger.setLevel(logging.INFO)
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)

    def load_processed_reports(self):
        """å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸ã®æƒ…å ±ã‚’èª­ã¿è¾¼ã‚€"""
        if self.processed_reports_file.exists():
            try:
                with open(self.processed_reports_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except json.JSONDecodeError:
                self.logger.error(f"å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {self.processed_reports_file}")
                return {}
        else:
            self.logger.info(f"å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™: {self.processed_reports_file}")
            return {}

    def save_processed_reports(self):
        """å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸ã®æƒ…å ±ã‚’ä¿å­˜ã™ã‚‹"""
        try:
            with open(self.processed_reports_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_reports, f, ensure_ascii=False, indent=2)
            self.logger.info(f"å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸æƒ…å ±ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.processed_reports_file}")
        except Exception as e:
            self.logger.error(f"å‡¦ç†æ¸ˆã¿å ±å‘Šæ›¸æƒ…å ±ã®ä¿å­˜ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")

    def is_already_processed(self, report_info):
        """
        å ±å‘Šæ›¸ãŒæ—¢ã«å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ã‚’åˆ¤å®š
        Args:
            report_info (dict): å ±å‘Šæ›¸æƒ…å ±
        Returns:
            bool: å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹
        """
        # å ±å‘Šæ›¸ã‚’ä¸€æ„ã«è­˜åˆ¥ã™ã‚‹ã‚­ãƒ¼ã‚’ç”Ÿæˆ
        report_id = self._generate_report_id(report_info)
        
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒä½¿ç”¨å¯èƒ½ãªå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if hasattr(self, 'db'):
            return self.db.is_already_processed(report_id)
        else:
            # å¾“æ¥ã®JSONæ–¹å¼
            if report_id in self.processed_reports:
                self.logger.info(f"ã“ã®å ±å‘Šæ›¸ã¯æ—¢ã«å‡¦ç†æ¸ˆã¿ã§ã™: {report_id}")
                return True
            return False

    def _generate_report_id(self, report_info):
        """
        å ±å‘Šæ›¸ã®ä¸€æ„è­˜åˆ¥å­ã‚’ç”Ÿæˆ
        Args:
            report_info (dict): å ±å‘Šæ›¸æƒ…å ±
        Returns:
            str: å ±å‘Šæ›¸ID
        """
        # ä¼æ¥­ã‚³ãƒ¼ãƒ‰ã€æå‡ºæ—¥ã€å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥ã€å ±å‘Šæ›¸ç¨®é¡ã€ä¿æœ‰è€…ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’ç”Ÿæˆ
        security_code = report_info.get('security_code', '')
        submission_date = report_info.get('submission_date', '')
        report_date = report_info.get('report_date', '')  # å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥ã‚’è¿½åŠ 
        report_type = report_info.get('report_type', '')
        holder_name = report_info.get('holder_name', '')
        
        # æ—¥æœ¬èªã®æ—¥ä»˜ã‹ã‚‰æ•°å­—ã®ã¿ã‚’æŠ½å‡º
        submission_numbers = re.sub(r'[^0-9]', '', submission_date)
        report_numbers = re.sub(r'[^0-9]', '', report_date)  # å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥ã®æ•°å­—ã‚’æŠ½å‡º
        
        return f"{security_code}_{submission_numbers}_{report_numbers}_{report_type}_{holder_name}"

    def mark_as_processed(self, report_info):
        """
        å ±å‘Šæ›¸ã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
        Args:
            report_info (dict): å ±å‘Šæ›¸æƒ…å ±
        """
        # SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒä½¿ç”¨å¯èƒ½ãªå ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if hasattr(self, 'db'):
            # report_idã‚’æ˜ç¤ºçš„ã«è¿½åŠ 
            report_info['report_id'] = self._generate_report_id(report_info)
            self.db.mark_as_processed(report_info)
        else:
            # å¾“æ¥ã®JSONæ–¹å¼
            report_id = self._generate_report_id(report_info)
            
            # å‡¦ç†æ—¥æ™‚ã‚’å«ã‚ã¦ä¿å­˜
            self.processed_reports[report_id] = {
                'processed_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'target_company': report_info.get('target_company', 'ä¸æ˜'),
                'security_code': report_info.get('security_code', 'ä¸æ˜'),
                'report_type': report_info.get('report_type', 'ä¸æ˜'),
                'holder_name': report_info.get('holder_name', 'ä¸æ˜'),
                'report_date': report_info.get('report_date', 'ä¸æ˜'),  # å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥ã‚’è¿½åŠ 
                'submission_date': report_info.get('submission_date', 'ä¸æ˜')  # æå‡ºæ—¥ã‚‚ä¿å­˜
            }
            
            # å¤‰æ›´ã‚’ä¿å­˜
            self.save_processed_reports()

    def find_latest_directories(self):
        """
        æœ€æ–°ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®šã™ã‚‹
        Returns:
            list: æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆï¼ˆæœ€æ–°é †ï¼‰
        """
        # åŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ã‚µãƒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å–å¾—
        all_dirs = [d for d in self.base_dir.iterdir() if d.is_dir()]
        
        # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæœ€æ–°ã®ã‚‚ã®ãŒå…ˆé ­ï¼‰
        latest_dirs = sorted(all_dirs, key=lambda d: d.stat().st_mtime, reverse=True)
        
        self.logger.info(f"æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®šã—ã¾ã—ãŸ: {[d.name for d in latest_dirs[:3]]}")
        return latest_dirs

    def find_all_public_docs(self):
        """
        ã™ã¹ã¦ã®PublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å†å¸°çš„ã«æ¤œç´¢
        Returns:
            list: è¦‹ã¤ã‹ã£ãŸPublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        """
        # **/ ã¯å†å¸°çš„ãªæ¤œç´¢ã‚’æ„å‘³ã™ã‚‹
        public_docs = list(self.base_dir.glob('**/PublicDoc'))
        self.logger.info(f"{len(public_docs)}å€‹ã®PublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
        return public_docs

    def parse_directory(self, specific_dir=None):
        """
        ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ã¦ã®XBRLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ï¼ˆæœ€æ–°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢ï¼‰
        Args:
            specific_dir (str, optional): ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šã™ã‚‹å ´åˆã®ãƒ‘ã‚¹
        Returns:
            list: å‡¦ç†çµæœã®ãƒªã‚¹ãƒˆ
        """
        try:
            results = []
            new_results = []  # æ–°è¦ã®å ±å‘Šæ›¸ã®ã¿ã‚’æ ¼ç´
            
            if specific_dir:
                # ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
                target_dir = Path(self.base_dir) / specific_dir if not Path(specific_dir).is_absolute() else Path(specific_dir)
                if target_dir.exists():
                    self.logger.info(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ä¸­: {target_dir}")
                    dirs_to_process = [target_dir]
                else:
                    self.logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_dir}")
                    return results, new_results
            else:
                # æŒ‡å®šãŒãªã„å ´åˆã¯æœ€æ–°ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†
                dirs_to_process = self.find_latest_directories()
                
                if not dirs_to_process:
                    self.logger.warning("å‡¦ç†å¯¾è±¡ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    return results, new_results
            
            # å„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®PublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
            for dir_path in dirs_to_process:
                self.logger.info(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ä¸­: {dir_path.name}")
                
                # PublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¤œç´¢
                for public_doc in dir_path.glob('**/PublicDoc'):
                    self.logger.info(f"PublicDocãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‡¦ç†ä¸­: {public_doc}")
                    
                    # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
                    header_files = list(public_doc.glob('*header*.htm*'))
                    honbun_files = list(public_doc.glob('*honbun*.htm*'))
                    
                    if header_files and honbun_files:
                        self.logger.info(f"ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«: {header_files[0].name}")
                        self.logger.info(f"æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«: {honbun_files[0].name}")
                        
                        # å„ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€åˆã®ã‚‚ã®ã‚’ä½¿ç”¨
                        result = self.parse_files(header_files[0], honbun_files[0])
                        if result:
                            # å‡¦ç†æ¸ˆã¿ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
                            if not self.is_already_processed(result):
                                # æœªå‡¦ç†ã®å ±å‘Šæ›¸ã‚’æ–°è¦ãƒªã‚¹ãƒˆã«è¿½åŠ 
                                new_results.append(result)
                                # å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
                                self.mark_as_processed(result)
                            
                            # ã™ã¹ã¦ã®çµæœã‚’å…¨ä½“ãƒªã‚¹ãƒˆã«è¿½åŠ ï¼ˆçµ±è¨ˆç”¨ï¼‰
                            results.append(result)
                            self.logger.info(f"å ±å‘Šæ›¸ã‚’å‡¦ç†ã—ã¾ã—ãŸ: {result['report_type']} - {result.get('target_company', 'ä¸æ˜')}")
            
            self.logger.info(f"åˆè¨ˆ{len(results)}ä»¶ã®å ±å‘Šæ›¸ã‚’å‡¦ç†ã—ã€ã†ã¡{len(new_results)}ä»¶ãŒæ–°è¦å ±å‘Šæ›¸ã§ã™")
            
            # çµæœã‚’è¿”ã™å‰ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹
            if hasattr(self, 'db') and hasattr(self, 'logger'):
                self.logger.info("å‡¦ç†å®Œäº†å¾Œã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã¾ã™")
                self.db.close()
            
            return results, new_results
        except Exception as e:
            self.logger.error(f"ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’é–‰ã˜ã‚‹
            if hasattr(self, 'db'):
                self.db.close()
            return [], []

    def parse_files(self, header_file, honbun_file):
        """
        ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
        Args:
            header_file (Path): ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            honbun_file (Path): æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        Returns:
            dict: è§£æçµæœ
        """
        try:
            # ãƒ˜ãƒƒãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æã—ã¦å ±å‘Šæ›¸ã®ç¨®é¡ã‚’åˆ¤å®š
            with open(header_file, 'r', encoding='utf-8') as f:
                header_soup = BeautifulSoup(f, 'html.parser')
                report_type = self._get_report_type(header_soup)

            # æœ¬æ–‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æ
            with open(honbun_file, 'r', encoding='utf-8') as f:
                honbun_soup = BeautifulSoup(f, 'html.parser')

            if report_type == "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸":
                return self._parse_large_volume_report(header_soup, honbun_soup)
            elif report_type == "å¤‰æ›´å ±å‘Šæ›¸":
                return self._parse_change_report(header_soup, honbun_soup)
            else:
                self.logger.warning(f"æœªå¯¾å¿œã®å ±å‘Šæ›¸ã‚¿ã‚¤ãƒ—: {report_type}")
                return None

        except Exception as e:
            self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {str(e)}")
            return None

    def _get_report_type(self, soup):
        """å ±å‘Šæ›¸ã®ç¨®é¡ã‚’åˆ¤å®š"""
        try:
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¤œç´¢ã—ã¦ã€Œæå‡ºæ›¸é¡ã€æ¬„ã‚’æ¢ã™
            table = soup.find('table')
            if table:
                for row in table.find_all('tr'):
                    cells = row.find_all('td')
                    if len(cells) >= 2:
                        # ã€Œæå‡ºæ›¸é¡ã€æ¬„ã‚’è¦‹ã¤ã‘ãŸå ´åˆ
                        if "æå‡ºæ›¸é¡" in cells[0].text:
                            document_type = cells[1].text.strip()
                            self.logger.info(f"æå‡ºæ›¸é¡ã®ç¨®é¡: {document_type}")
                            
                            # ã€Œå¤‰æ›´å ±å‘Šæ›¸ã€ã¨ã„ã†æ–‡å­—ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°å¤‰æ›´å ±å‘Šæ›¸
                            if "å¤‰æ›´å ±å‘Šæ›¸" in document_type:
                                return "å¤‰æ›´å ±å‘Šæ›¸"
                            # ãã‚Œä»¥å¤–ã¯å¤§é‡ä¿æœ‰å ±å‘Šæ›¸ï¼ˆã¾ãŸã¯ä»–ã®ç¨®é¡ï¼‰
                            else:
                                return "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸"
            
            # æå‡ºæ›¸é¡æ¬„ãŒè¦‹ã¤ã‹ã‚‰ãªã‹ã£ãŸå ´åˆã¯ã‚¿ã‚¤ãƒˆãƒ«ã§åˆ¤æ–­ï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
            title = soup.find('title').text
            if "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸" in title and "å¤‰æ›´å ±å‘Šæ›¸" not in title:
                return "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸"
            elif "å¤‰æ›´å ±å‘Šæ›¸" in title:
                return "å¤‰æ›´å ±å‘Šæ›¸"
            
            # åˆ¤å®šã§ããªã„å ´åˆ
            self.logger.warning("å ±å‘Šæ›¸ç¨®é¡ã®åˆ¤å®šãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã€Œå¤§é‡ä¿æœ‰å ±å‘Šæ›¸ã€ã¨ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚")
            return "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸"
        except Exception as e:
            self.logger.error(f"å ±å‘Šæ›¸ç¨®é¡ã®åˆ¤å®šä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return "ä¸æ˜"

    def _parse_large_volume_report(self, header_soup, honbun_soup):
        """å¤§é‡ä¿æœ‰å ±å‘Šæ›¸ã®è§£æ"""
        try:
            # æå‡ºè€…ã®æƒ…å ±ã‚’å–å¾—
            filer_info = self._get_filer_info(header_soup)
            
            # æœ¬æ–‡ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            data = {
                "report_type": "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸",
                "target_company": self._get_text_by_id(honbun_soup, "T0100000000101"),
                "security_code": self._get_text_by_id(honbun_soup, "T0100000000201"),
                "holder_name": self._get_text_by_id(honbun_soup, "T0201010100401") or filer_info.get("name"),
                "holding_ratio": self._get_text_by_id(honbun_soup, "T0201040200201"),
                "report_date": filer_info.get("report_date"),
                "submission_date": filer_info.get("submission_date"),
                "shares_held": self._get_text_by_id(honbun_soup, "T0201040101401"),
                "purpose": self._get_text_by_id(honbun_soup, "T0201020000101")
            }
            
            # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            data = self._clean_data(data)
            
            return data
        except Exception as e:
            self.logger.error(f"å¤§é‡ä¿æœ‰å ±å‘Šæ›¸ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def _parse_change_report(self, header_soup, honbun_soup):
        """å¤‰æ›´å ±å‘Šæ›¸ã®è§£æ"""
        try:
            # æå‡ºè€…ã®æƒ…å ±ã‚’å–å¾—
            filer_info = self._get_filer_info(header_soup)
            
            # æœ¬æ–‡ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            data = {
                "report_type": "å¤‰æ›´å ±å‘Šæ›¸",
                "target_company": self._get_text_by_id(honbun_soup, "T0100000000101"),
                "security_code": self._get_text_by_id(honbun_soup, "T0100000000201"),
                "holder_name": self._get_text_by_id(honbun_soup, "T0201010100401") or filer_info.get("name"),
                "holding_ratio_before": self._get_text_by_id(honbun_soup, "T0201040200301"),
                "holding_ratio_after": self._get_text_by_id(honbun_soup, "T0201040200201"),
                "report_date": filer_info.get("report_date"),
                "submission_date": filer_info.get("submission_date"),
                "shares_held": self._get_text_by_id(honbun_soup, "T0201040101401"),
                "purpose": self._get_text_by_id(honbun_soup, "T0201020000101")
            }
            
            # ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
            data = self._clean_data(data)
            
            return data
        except Exception as e:
            self.logger.error(f"å¤‰æ›´å ±å‘Šæ›¸ã®è§£æä¸­ã«ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

    def _get_filer_info(self, header_soup):
        """ãƒ˜ãƒƒãƒ€ãƒ¼ã‹ã‚‰æå‡ºè€…æƒ…å ±ã‚’å–å¾—"""
        info = {}
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’å–å¾—
        table = header_soup.find('table')
        if table:
            rows = table.find_all('tr')
            
            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 2:
                    item_cell = cells[0].text.strip()
                    value_cell = cells[1].text.strip()
                    
                    if "æ°ååˆã¯åç§°" in item_cell:
                        info["name"] = value_cell
                    elif "å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥" in item_cell:
                        info["report_date"] = value_cell
                    elif "æå‡ºæ—¥" in item_cell:
                        info["submission_date"] = value_cell
        
        return info

    def _get_text_by_id(self, soup, id_value):
        """æŒ‡å®šã•ã‚ŒãŸIDã‚’æŒã¤è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        element = soup.find(id=id_value)
        return element.text.strip() if element else None

    def _clean_data(self, data):
        """ãƒ‡ãƒ¼ã‚¿ã®æ•´å½¢ã¨æ•°å€¤ã®æŠ½å‡º"""
        cleaned_data = {}
        
        for key, value in data.items():
            if value is None:
                cleaned_data[key] = None
                continue
                
            # ä¿æœ‰å‰²åˆã‹ã‚‰æ•°å€¤ã‚’æŠ½å‡º
            if "holding_ratio" in key and value:
                # æ•°å€¤ã‚’æŠ½å‡º (ä¾‹: "5.31%" -> "5.31")
                match = re.search(r'(\d+\.\d+|\d+)', value)
                if match:
                    cleaned_data[key] = match.group(1)
                else:
                    cleaned_data[key] = value
            else:
                cleaned_data[key] = value
                
        return cleaned_data

    def get_formatted_result(self, result):
        """çµæœã‚’æ•´å½¢ã—ã¦è¡¨ç¤ºç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        if not result:
            return "çµæœãŒã‚ã‚Šã¾ã›ã‚“"
            
        if result["report_type"] == "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸":
            text = f"ã€å¤§é‡ä¿æœ‰å ±å‘Šæ›¸ã€‘\n"
            text += f"å¯¾è±¡ä¼æ¥­: {result.get('target_company', 'ä¸æ˜')} ({result.get('security_code', 'ä¸æ˜')})\n"
            text += f"ä¿æœ‰è€…: {result.get('holder_name', 'ä¸æ˜')}\n"
            text += f"ä¿æœ‰å‰²åˆ: {result.get('holding_ratio', 'ä¸æ˜')}%\n"
            text += f"ä¿æœ‰æ ªå¼æ•°: {result.get('shares_held', 'ä¸æ˜')}æ ª\n"
            text += f"å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥: {result.get('report_date', 'ä¸æ˜')}\n"
            text += f"æå‡ºæ—¥: {result.get('submission_date', 'ä¸æ˜')}\n"
            text += f"ç›®çš„: {result.get('purpose', 'ä¸æ˜')}"
        else:
            text = f"ã€å¤‰æ›´å ±å‘Šæ›¸ã€‘\n"
            text += f"å¯¾è±¡ä¼æ¥­: {result.get('target_company', 'ä¸æ˜')} ({result.get('security_code', 'ä¸æ˜')})\n"
            text += f"ä¿æœ‰è€…: {result.get('holder_name', 'ä¸æ˜')}\n"
            text += f"å¤‰æ›´å‰ä¿æœ‰å‰²åˆ: {result.get('holding_ratio_before', 'ä¸æ˜')}%\n"
            text += f"å¤‰æ›´å¾Œä¿æœ‰å‰²åˆ: {result.get('holding_ratio_after', 'ä¸æ˜')}%\n"
            text += f"ä¿æœ‰æ ªå¼æ•°: {result.get('shares_held', 'ä¸æ˜')}æ ª\n"
            text += f"å ±å‘Šç¾©å‹™ç™ºç”Ÿæ—¥: {result.get('report_date', 'ä¸æ˜')}\n"
            text += f"æå‡ºæ—¥: {result.get('submission_date', 'ä¸æ˜')}\n"
            text += f"ç›®çš„: {result.get('purpose', 'ä¸æ˜')}"
            
        return text

    def get_line_message(self, result):
        """
        LINEç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
        Args:
            result (dict): è§£æçµæœ
        Returns:
            str: LINEç”¨ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        """
        if not result:
            return "çµæœãŒã‚ã‚Šã¾ã›ã‚“"
        
        # çµµæ–‡å­—ã‚’è¿½åŠ ã—ãŸã‚ˆã‚Šè¦‹ã‚„ã™ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ä½œæˆ
        if result["report_type"] == "å¤§é‡ä¿æœ‰å ±å‘Šæ›¸":
            message = f"ğŸ“Š å¤§é‡ä¿æœ‰å ±å‘Šæ›¸\n\n"
            message += f"ğŸ¢ {result.get('target_company', 'ä¸æ˜')} ({result.get('security_code', 'ä¸æ˜')})\n"
            message += f"ğŸ‘¤ {result.get('holder_name', 'ä¸æ˜')}\n"
            message += f"ğŸ“ˆ ä¿æœ‰å‰²åˆ: {result.get('holding_ratio', 'ä¸æ˜')}%\n"
            message += f"ğŸ“ {result.get('shares_held', 'ä¸æ˜')}æ ª\n"
            message += f"ğŸ“… {result.get('report_date', 'ä¸æ˜')}\n"
            message += f"ğŸ” ç›®çš„: {result.get('purpose', 'ä¸æ˜')}"
        else:
            # å¤‰æ›´å‰å¾Œã®å‰²åˆã®å·®ã‚’è¨ˆç®—
            before = float(result.get('holding_ratio_before', '0').replace('%', '')) if result.get('holding_ratio_before') else 0
            after = float(result.get('holding_ratio_after', '0').replace('%', '')) if result.get('holding_ratio_after') else 0
            diff = after - before
            diff_str = f"+{diff:.2f}%" if diff > 0 else f"{diff:.2f}%"
            
            message = f"ğŸ“Š å¤‰æ›´å ±å‘Šæ›¸\n\n"
            message += f"ğŸ¢ {result.get('target_company', 'ä¸æ˜')} ({result.get('security_code', 'ä¸æ˜')})\n"
            message += f"ğŸ‘¤ {result.get('holder_name', 'ä¸æ˜')}\n"
            message += f"ğŸ“‰ å¤‰æ›´å‰: {result.get('holding_ratio_before', 'ä¸æ˜')}%\n"
            message += f"ğŸ“ˆ å¤‰æ›´å¾Œ: {result.get('holding_ratio_after', 'ä¸æ˜')}% ({diff_str})\n"
            message += f"ğŸ“ {result.get('shares_held', 'ä¸æ˜')}æ ª\n"
            message += f"ğŸ“… {result.get('report_date', 'ä¸æ˜')}\n"
            message += f"ğŸ” ç›®çš„: {result.get('purpose', 'ä¸æ˜')}"
        
        return message

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    # EDINETã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®šï¼ˆhikariget.pyã¨åŒã˜å ´æ‰€ã‚’æƒ³å®šï¼‰
    current_dir = Path.cwd()
    target_dir = current_dir / "edinet_downloads"

    if not target_dir.exists():
        logger.error(f"æŒ‡å®šã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {target_dir}")
        return

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‚’å‡¦ç†ã™ã‚‹å ´åˆ
    import sys
    specific_dir = None
    
    if len(sys.argv) > 1:
        # ç¬¬1å¼•æ•°ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦æ‰±ã†
        specific_dir = sys.argv[1]
        logger.info(f"ç‰¹å®šã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒæŒ‡å®šã•ã‚Œã¾ã—ãŸ: {specific_dir}")

    # ã¾ãšZIPãƒ•ã‚¡ã‚¤ãƒ«ã®è§£å‡å‡¦ç†ã‚’è¡Œã†
    unzipper = EdinetUnzipper(target_dir)
    success, failure = unzipper.process_all_zips()

    if success + failure > 0:
        logger.info("å…¨ã¦ã®ZIPè§£å‡å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
    else:
        logger.info("å‡¦ç†å¯¾è±¡ã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    # ãƒ‘ãƒ¼ã‚µãƒ¼ã«ã‚ˆã‚‹è§£æå‡¦ç†
    parser = EdinetParser(target_dir)
    all_results, new_results = parser.parse_directory(specific_dir)

    # å…¨ã¦ã®çµæœã®è¡¨ç¤ºï¼ˆçµ±è¨ˆæƒ…å ±ç”¨ï¼‰
    if all_results:
        logger.info(f"åˆè¨ˆ{len(all_results)}ä»¶ã®å ±å‘Šæ›¸ã‚’å‡¦ç†ã—ã€ã†ã¡{len(new_results)}ä»¶ãŒæ–°è¦å ±å‘Šæ›¸ã§ã™")
        
        # æ–°è¦å ±å‘Šæ›¸ã®ã¿ã‚’å‡¦ç†ï¼ˆLINEé€šçŸ¥ãªã©ï¼‰
        if new_results:
            logger.info(f"ä»¥ä¸‹ã®{len(new_results)}ä»¶ã®æ–°è¦å ±å‘Šæ›¸ã‚’é€šçŸ¥ã—ã¾ã™")
            
            # é€šçŸ¥ç”¨ã«notifier.pyã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            try:
                from notifier import send_message, send_line_message
                
                for result in new_results:
                    formatted_text = parser.get_formatted_result(result)
                    print(formatted_text)
                    print("---")
                    
                    # LINEç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
                    line_message = parser.get_line_message(result)
                    logger.info(f"LINEç”¨ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ:\n{line_message}")
                    
                    # LINEã«é€ä¿¡
                    send_message(line_message)
                    logger.info("LINEé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸ")
                
            except ImportError as e:
                logger.error(f"notifierãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                logger.error("LINEé€šçŸ¥æ©Ÿèƒ½ã¯ä½¿ç”¨ã§ãã¾ã›ã‚“")
        else:
            logger.info("æ–°è¦ã®å ±å‘Šæ›¸ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    else:
        logger.info("å‡¦ç†ã•ã‚ŒãŸå ±å‘Šæ›¸ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main() 